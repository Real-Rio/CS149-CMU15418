# CS149 2023fall assignment记录

课程主页：https://gfxcourses.stanford.edu/cs149/fall23/

视频我是在 b 站看的 2018 年的，作业写的是 2023 年的，实测这几年的 assignment 都没有什么变化



## Assignment  1： Performance Analysis on a Quad-Core CPU

### Environment Setup

我是用的 m1 pro Mac ，8 核 CPU，8 个线程



### Program 1：Parallel Fractal Generation Using Threads

需要在 `workerTreadStart`中调用`mandelbrotSerial`函数，至于具体如何将任务分配给每一个线程，这是这个 program 希望我们能自己探索的

```go
int id = args->threadId;
int totalRows = args->height / args->numThreads;
if (args->height % args->numThreads != 0)
{
  totalRows++;
}
int startRow = id * totalRows;
if (startRow >= args->height)
{
  return;
}
if (startRow + totalRows > args->height)
{
  totalRows = args->height - startRow;
}
mandelbrotSerial(args->x0, args->y0, args->x1, args->y1, args->width, args->height, startRow, totalRows, args->maxIterations, args->output);
```

一开始自然而然地想到按照分块进行划分，即每个线程按照图片从上到下，各自负责一段区域的计算，最后算出来的加速比与线程数量关系如下图
<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/Figure_1.png" alt="Figure_1" style="zoom:72%;" />

可以看到在线程数为 3 的时候，加速比竟然比线程数为 2 时还要小，原因是 3 个线程任务分配不均衡，View1 图像计算任务大部分集中在中间区域，所以 Thread1 相比其他 2 个线程计算量更大，最后可能其他两个线程早都执行完了还需要等待 Thread1 的完成

<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240308140805251.png" alt="image-20240308140805251" style="zoom: 25%;" />

```
Thread 2 time: 83.439 ms
Thread 0 time: 83.503 ms
Thread 1 time: 255.239 ms
```

为了让每个线程的计算量接近，那么就不能根据图像本身进行划分，可以以更小的细粒度，比如每个线程依次对每个像素进行运算，修改`mandelbrotSerial`的代码为

```go
void mandelbrotSerial2(
    float x0, float y0, float x1, float y1,
    int width, int height,
    int startIdx, int numThreads,
    int maxIterations,
    int output[])
{
  float dx = (x1 - x0) / width;
  float dy = (y1 - y0) / height;

  for (int idx = startIdx; idx < width * height; idx += numThreads)
  {
    int i = idx % width;
    int j = idx / width;
    float x = x0 + i * dx;
    float y = y0 + j * dy;

    output[idx] = mandel(x, y, maxIterations);
  }
}

```

然后在`workerThreadStart`中调用

```go
mandelbrotSerial2(args->x0, args->y0, args->x1, args->y1, args->width, args->height, args->threadId, args->numThreads, args->maxIterations, args->output);

```

改进后的加速比为
<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/Figure_2.png" alt="Figure_2" style="zoom:72%;" />

可以看出在线程数为 8 之前，加速比上升得非常均匀，8 之后是因为 CPU 只支持 8 线程并行，所以加速比不升反降

作图的 python 程序为

```python
import subprocess
import re
import matplotlib.pyplot as plt

# 不同的numThreads值
numThreads_values = list(range(1, 17))  # 根据需要修改

# 存储加速比的列表
speedup_values = []

# 调用命令行程序，并获取输出
for numThreads in numThreads_values:
    command = f"./mandelbrot -t {numThreads}"
    output = subprocess.check_output(command, shell=True, encoding='utf-8')

    # 从输出中提取加速比
    match = re.search(r"\(([\d.]+)x speedup", output)
    if match:
        speedup = float(match.group(1))
        speedup_values.append(speedup)
    else:
        speedup_values.append(1.0)  # 如果无法提取加速比，则默认为1.0

# 绘制折线图
plt.plot(numThreads_values, speedup_values, marker='o')

# 添加图表标题和轴标签
plt.title('Mandelbrot Speedup')
plt.xlabel('numThreads')
plt.ylabel('Speedup')

# 显示图表
plt.show()

```



### Program 2：Vectorizing Code Using SIMD Intrinsics

program2 只需学习给出的样例程序，然后依葫芦画瓢即可

```go
void clampedExpVector(float *values, int *exponents, float *output, int N)
{
  __cs149_vec_float x;
  __cs149_vec_int y;
  __cs149_vec_float result;
  __cs149_vec_int allOneInt = _cs149_vset_int(1);
  __cs149_vec_int zeroInt = _cs149_vset_int(0);
  __cs149_vec_float maxVal = _cs149_vset_float(9.999999f);
  __cs149_mask maskAll, maskIsNotZero, maskIsZero;

  for (int i = 0; i < N; i += VECTOR_WIDTH)
  {
    if (i + VECTOR_WIDTH > N)
    {
      maskAll = _cs149_init_ones(N - i);
    }
    else
    {
      maskAll = _cs149_init_ones();
    }
    maskIsZero = _cs149_init_ones(0);
    
    _cs149_vload_float(x, values + i, maskAll); // x = values[i];
    _cs149_vmove_float(result, x, maskAll);
    _cs149_vload_int(y, exponents + i, maskAll); // y = exponents[i];

    _cs149_veq_int(maskIsZero, y, zeroInt, maskAll); // if (y == 0) {
    _cs149_vset_float(result, 1.f, maskIsZero);      //   output[i] = 1.f;

    maskIsNotZero = _cs149_mask_not(maskIsZero); // } else {

    while (1)
    {
      _cs149_vsub_int(y, y, allOneInt, maskIsNotZero); // count = y - 1

      _cs149_vgt_int(maskIsNotZero, y, zeroInt, maskIsNotZero); // if (count > 0) {
      int num = _cs149_cntbits(maskIsNotZero);
      if (num == 0)
      {
        break;
      }
      _cs149_vmult_float(result, result, x, maskIsNotZero); //   resulte *= x;
    }

    maskIsZero = _cs149_init_ones(0);
    _cs149_vgt_float(maskIsZero, result, maxVal, maskAll); // if (result > 9.999999f) {
    _cs149_vset_float(result, 9.999999f, maskIsZero);      //   result = 9.999999f;

    _cs149_vstore_float(output + i, result, maskAll);
  }
}
```



### Program 3 Parallel Fractal Generation Using ISPC

**Part 1**

根据作业 github 界面上的提示，由于我用的 m 芯片 mac，将 Makefile 中 ISPC 的 flag 改为`ISPCFLAGS=-O3 --target=neon-i32x8 --arch=aarch64 --opt=disable-fma --pic`

```
[mandelbrot serial]:            [208.261] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [58.091] ms
Wrote image file mandelbrot-ispc.ppm
                                (3.59x speedup from ISPC)
```

**Part 2**

task数目为 2

```
[mandelbrot serial]:            [213.240] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [58.859] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [29.746] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.62x speedup from ISPC)
                                (7.17x speedup from task ISPC)
```

task数目为 8 时

```
[mandelbrot serial]:            [208.007] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [57.842] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [14.501] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.60x speedup from ISPC)
                                (14.34x speedup from task ISPC)
```

task数目为 18 时

```
[mandelbrot serial]:            [208.110] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [57.966] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [11.256] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.59x speedup from ISPC)
                                (18.49x speedup from task ISPC)
```

为什么 task 超过了 8 个核心数，加速比还能继续提高呢？在[课件](http://15418.courses.cs.cmu.edu/spring2016/lecture/progabstractions/slide_007)里有一段讨论，里面引用了 ispc 文档里的话

> In general, one should launch many more tasks than there are processors in the system to ensure good load-balancing, but not so many that the overhead of scheduling and running tasks dominates the computation

这样做的目的是为了不同线程尽可能负载均衡，和 Program1 中的优化类似。比如有 4 个核心，分配了 4 个 task，执行时间分别为 1ms、2ms、10ms、1ms，那么总时间由于木桶效应就约为 10ms，而如果我们分配 8 个 task，那么每个 task 执行的计算量相比 4 个 task 是减少的，就算可能分配不均，“木桶”中的短板与长板的差距也会缩小，最后反映到执行的总时间会减少。当然task 数目也不是越多越好，这是一个 tradeoff，如果 task 间切换的开销大于负载均衡的优化，那么就不应该继续增加 task 的数目了

> Program 4 和 5 感觉没啥好写的，直接跳过。Program 6 要从斯坦福服务器获取数据集也懒得写了



## Assignment 2：Building A Task Execution Library from the Ground Up

### 

### Part A

做这个 assignment 之前一定要好好看一些 [readme](https://github.com/stanford-cs149/asst2)，把整个项目的文件结构搞清楚再开始动手。readme 中贴心的给出了 *what you need to do*，实现的难度循序渐进

**step 1**

Step 1主要考察多线程的使用，只需将 tasks 尽可能平均分配给每一个线程即可

```c++
int tasks_per_thread = (num_total_tasks + num_threads_ - 1) / num_threads_;
for (int i = 0; i < num_threads_; i++)
{
  int start = i * tasks_per_thread;
  int end = std::min((i + 1) * tasks_per_thread, num_total_tasks);
  threads_[i] = std::thread([runnable, start, end, num_total_tasks]()
                            {
                              for (int j = start; j < end; j++)
                              {
                                runnable->runTask(j, num_total_tasks);
                              } });
}
for (int i = 0; i < num_threads_; i++)
{
  threads_[i].join();
}
```

**step 2 step 3**

step 2 和 3 我合在一起写了，step 2 要求用线程池 ，step 3 要求用条件变量。使用线程池的好处是减少线程创建销毁带来的开销，每个线程在完成任务之后并不退出，而是循环地检查任务池是否有新的任务，如果没有则利用条件变量 sleep 释放资源，直到任务池有新增任务唤醒了休眠的线程，线程才会执行

有一个需要思考的问题是，使用线程池之后主线程怎么知道任务什么时候完成呢？在 step 1 中，可以使用 `thread.join()`函数等待所有线程退出，但在线程池中，线程一直处于循环，该方法就失效了。解决办法也是使用条件变量，每新增一个任务 cnt 就加 1，每完成一个任务 cnt 减 1，减到 0 后线程池就唤醒主线程的条件变量

线程池实现

```c++
class ThreadPool
{
public:
    ThreadPool(size_t numThreads) : taskCount(0), stop(false)
    {
        for (size_t i = 0; i < numThreads; ++i)
        {
            threads.emplace_back([this]
                                 {
                while (true) {
                    std::function<void()> task;

                    {
                        std::unique_lock<std::mutex> lock(mutex);
                        condition.wait(lock, [this] { return stop || !tasks.empty(); });
                        if (stop ) // 线程池即将销毁
                            return;
                        task = std::move(tasks.front());
                        tasks.pop();
                    }

                    task();

                     {
                        std::unique_lock<std::mutex> lock(mutexCount);
                        --taskCount;
                        if (taskCount == 0) {
                            conditionCount.notify_all();
                        }
                    }
                } });
        }
    }

    void waitAllTasks()
    {
        std::unique_lock<std::mutex> lock(mutexCount);
        conditionCount.wait(lock, [this]
                            { return taskCount == 0; });
    }

    template <typename Func, typename... Args>
    void enqueue(Func &&func, Args &&...args)
    {
        {
            std::unique_lock<std::mutex> lock(mutexCount);
            ++taskCount;
        }

        {
            std::unique_lock<std::mutex> lock(mutex);
            tasks.emplace([func, args...]
                          { func(args...); });
        }
        condition.notify_one();
    }

    ~ThreadPool()
    {
        {
            std::unique_lock<std::mutex> lock(mutex);
            stop = true;
        }
        condition.notify_all();
        for (std::thread &thread : threads)
            thread.join();
    }

private:
    std::vector<std::thread> threads;
    std::queue<std::function<void()>> tasks;

    std::mutex mutex;
    std::condition_variable condition;

    size_t taskCount;
    std::condition_variable conditionCount;
    std::mutex mutexCount;

    bool stop;
};
```

主线程

```c++
int tasks_per_thread = (num_total_tasks + num_threads_ - 1) / num_threads_;
    for (int i = 0; i < num_threads_; i++)
    {
        int start = i * tasks_per_thread;
        int end = std::min((i + 1) * tasks_per_thread, num_total_tasks);
        pool_->enqueue([runnable, start, end, num_total_tasks]()
                       {
            for (int j = start; j < end; j++)
            {
                runnable->runTask(j, num_total_tasks);
            } });
    }
    pool_->waitAllTasks();
```

