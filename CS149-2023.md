# CS149 2023fall assignment记录

课程主页：https://gfxcourses.stanford.edu/cs149/fall23/

视频我是在 b 站看的 2018 年的，作业写的是 2023 年的，实测这几年的 assignment 都没有什么变化



## Assignment  1： Performance Analysis on a Quad-Core CPU

### Environment Setup

我是用的 m1 pro Mac ，8 核 CPU，8 个线程



### Program 1：Parallel Fractal Generation Using Threads

需要在 `workerTreadStart`中调用`mandelbrotSerial`函数，至于具体如何将任务分配给每一个线程，这是这个 program 希望我们能自己探索的

```go
int id = args->threadId;
int totalRows = args->height / args->numThreads;
if (args->height % args->numThreads != 0)
{
  totalRows++;
}
int startRow = id * totalRows;
if (startRow >= args->height)
{
  return;
}
if (startRow + totalRows > args->height)
{
  totalRows = args->height - startRow;
}
mandelbrotSerial(args->x0, args->y0, args->x1, args->y1, args->width, args->height, startRow, totalRows, args->maxIterations, args->output);
```

一开始自然而然地想到按照分块进行划分，即每个线程按照图片从上到下，各自负责一段区域的计算，最后算出来的加速比与线程数量关系如下图
<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/Figure_1.png" alt="Figure_1" style="zoom:72%;" />

可以看到在线程数为 3 的时候，加速比竟然比线程数为 2 时还要小，原因是 3 个线程任务分配不均衡，View1 图像计算任务大部分集中在中间区域，所以 Thread1 相比其他 2 个线程计算量更大，最后可能其他两个线程早都执行完了还需要等待 Thread1 的完成

<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240308140805251.png" alt="image-20240308140805251" style="zoom: 25%;" />

```
Thread 2 time: 83.439 ms
Thread 0 time: 83.503 ms
Thread 1 time: 255.239 ms
```

为了让每个线程的计算量接近，那么就不能根据图像本身进行划分，可以以更小的细粒度，比如每个线程依次对每个像素进行运算，修改`mandelbrotSerial`的代码为

```go
void mandelbrotSerial2(
    float x0, float y0, float x1, float y1,
    int width, int height,
    int startIdx, int numThreads,
    int maxIterations,
    int output[])
{
  float dx = (x1 - x0) / width;
  float dy = (y1 - y0) / height;

  for (int idx = startIdx; idx < width * height; idx += numThreads)
  {
    int i = idx % width;
    int j = idx / width;
    float x = x0 + i * dx;
    float y = y0 + j * dy;

    output[idx] = mandel(x, y, maxIterations);
  }
}

```

然后在`workerThreadStart`中调用

```go
mandelbrotSerial2(args->x0, args->y0, args->x1, args->y1, args->width, args->height, args->threadId, args->numThreads, args->maxIterations, args->output);

```

改进后的加速比为
<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/Figure_2.png" alt="Figure_2" style="zoom:72%;" />

可以看出在线程数为 8 之前，加速比上升得非常均匀，8 之后是因为 CPU 只支持 8 线程并行，所以加速比不升反降

作图的 python 程序为

```python
import subprocess
import re
import matplotlib.pyplot as plt

# 不同的numThreads值
numThreads_values = list(range(1, 17))  # 根据需要修改

# 存储加速比的列表
speedup_values = []

# 调用命令行程序，并获取输出
for numThreads in numThreads_values:
    command = f"./mandelbrot -t {numThreads}"
    output = subprocess.check_output(command, shell=True, encoding='utf-8')

    # 从输出中提取加速比
    match = re.search(r"\(([\d.]+)x speedup", output)
    if match:
        speedup = float(match.group(1))
        speedup_values.append(speedup)
    else:
        speedup_values.append(1.0)  # 如果无法提取加速比，则默认为1.0

# 绘制折线图
plt.plot(numThreads_values, speedup_values, marker='o')

# 添加图表标题和轴标签
plt.title('Mandelbrot Speedup')
plt.xlabel('numThreads')
plt.ylabel('Speedup')

# 显示图表
plt.show()

```



### Program 2：Vectorizing Code Using SIMD Intrinsics

program2 只需学习给出的样例程序，然后依葫芦画瓢即可

```go
void clampedExpVector(float *values, int *exponents, float *output, int N)
{
  __cs149_vec_float x;
  __cs149_vec_int y;
  __cs149_vec_float result;
  __cs149_vec_int allOneInt = _cs149_vset_int(1);
  __cs149_vec_int zeroInt = _cs149_vset_int(0);
  __cs149_vec_float maxVal = _cs149_vset_float(9.999999f);
  __cs149_mask maskAll, maskIsNotZero, maskIsZero;

  for (int i = 0; i < N; i += VECTOR_WIDTH)
  {
    if (i + VECTOR_WIDTH > N)
    {
      maskAll = _cs149_init_ones(N - i);
    }
    else
    {
      maskAll = _cs149_init_ones();
    }
    maskIsZero = _cs149_init_ones(0);
    
    _cs149_vload_float(x, values + i, maskAll); // x = values[i];
    _cs149_vmove_float(result, x, maskAll);
    _cs149_vload_int(y, exponents + i, maskAll); // y = exponents[i];

    _cs149_veq_int(maskIsZero, y, zeroInt, maskAll); // if (y == 0) {
    _cs149_vset_float(result, 1.f, maskIsZero);      //   output[i] = 1.f;

    maskIsNotZero = _cs149_mask_not(maskIsZero); // } else {

    while (1)
    {
      _cs149_vsub_int(y, y, allOneInt, maskIsNotZero); // count = y - 1

      _cs149_vgt_int(maskIsNotZero, y, zeroInt, maskIsNotZero); // if (count > 0) {
      int num = _cs149_cntbits(maskIsNotZero);
      if (num == 0)
      {
        break;
      }
      _cs149_vmult_float(result, result, x, maskIsNotZero); //   resulte *= x;
    }

    maskIsZero = _cs149_init_ones(0);
    _cs149_vgt_float(maskIsZero, result, maxVal, maskAll); // if (result > 9.999999f) {
    _cs149_vset_float(result, 9.999999f, maskIsZero);      //   result = 9.999999f;

    _cs149_vstore_float(output + i, result, maskAll);
  }
}
```



### Program 3 Parallel Fractal Generation Using ISPC

**Part 1**

根据作业 github 界面上的提示，由于我用的 m 芯片 mac，将 Makefile 中 ISPC 的 flag 改为`ISPCFLAGS=-O3 --target=neon-i32x8 --arch=aarch64 --opt=disable-fma --pic`

```
[mandelbrot serial]:            [208.261] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [58.091] ms
Wrote image file mandelbrot-ispc.ppm
                                (3.59x speedup from ISPC)
```

**Part 2**

task数目为 2

```
[mandelbrot serial]:            [213.240] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [58.859] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [29.746] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.62x speedup from ISPC)
                                (7.17x speedup from task ISPC)
```

task数目为 8 时

```
[mandelbrot serial]:            [208.007] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [57.842] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [14.501] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.60x speedup from ISPC)
                                (14.34x speedup from task ISPC)
```

task数目为 18 时

```
[mandelbrot serial]:            [208.110] ms
Wrote image file mandelbrot-serial.ppm
[mandelbrot ispc]:              [57.966] ms
Wrote image file mandelbrot-ispc.ppm
[mandelbrot multicore ispc]:    [11.256] ms
Wrote image file mandelbrot-task-ispc.ppm
                                (3.59x speedup from ISPC)
                                (18.49x speedup from task ISPC)
```

为什么 task 超过了 8 个核心数，加速比还能继续提高呢？在[课件](http://15418.courses.cs.cmu.edu/spring2016/lecture/progabstractions/slide_007)里有一段讨论，里面引用了 ispc 文档里的话

> In general, one should launch many more tasks than there are processors in the system to ensure good load-balancing, but not so many that the overhead of scheduling and running tasks dominates the computation

这样做的目的是为了不同线程尽可能负载均衡，和 Program1 中的优化类似。比如有 4 个核心，分配了 4 个 task，执行时间分别为 1ms、2ms、10ms、1ms，那么总时间由于木桶效应就约为 10ms，而如果我们分配 8 个 task，那么每个 task 执行的计算量相比 4 个 task 是减少的，就算可能分配不均，“木桶”中的短板与长板的差距也会缩小，最后反映到执行的总时间会减少。当然task 数目也不是越多越好，这是一个 tradeoff，如果 task 间切换的开销大于负载均衡的优化，那么就不应该继续增加 task 的数目了

> Program 4 和 5 感觉没啥好写的，直接跳过。Program 6 要从斯坦福服务器获取数据集也懒得写了



## Assignment 2：Building A Task Execution Library from the Ground Up

### Part A

做这个 assignment 之前一定要好好看一些 [readme](https://github.com/stanford-cs149/asst2)，把整个项目的文件结构搞清楚再开始动手。readme 中贴心的给出了 *what you need to do*，实现的难度循序渐进

**step 1**

Step 1主要考察多线程的使用，只需将 tasks 尽可能平均分配给每一个线程即可

```c++
int tasks_per_thread = (num_total_tasks + num_threads_ - 1) / num_threads_;
for (int i = 0; i < num_threads_; i++)
{
  int start = i * tasks_per_thread;
  int end = std::min((i + 1) * tasks_per_thread, num_total_tasks);
  threads_[i] = std::thread([runnable, start, end, num_total_tasks]()
                            {
                              for (int j = start; j < end; j++)
                              {
                                runnable->runTask(j, num_total_tasks);
                              } });
}
for (int i = 0; i < num_threads_; i++)
{
  threads_[i].join();
}
```

**step 2 step 3**

step 2 和 3 我合在一起写了，step 2 要求用线程池 ，step 3 要求用条件变量。使用线程池的好处是减少线程创建销毁带来的开销，每个线程在完成任务之后并不退出，而是循环地检查任务池是否有新的任务，如果没有则利用条件变量 sleep 释放资源，直到任务池有新增任务唤醒了休眠的线程，线程才会执行

有一个需要思考的问题是，使用线程池之后主线程怎么知道任务什么时候完成呢？在 step 1 中，可以使用 `thread.join()`函数等待所有线程退出，但在线程池中，线程一直处于循环，该方法就失效了。解决办法也是使用条件变量，每新增一个任务 cnt 就加 1，每完成一个任务 cnt 减 1，减到 0 后线程池就唤醒主线程的条件变量

线程池实现

```c++
class ThreadPool
{
public:
    ThreadPool(size_t numThreads) : taskCount(0), stop(false)
    {
        for (size_t i = 0; i < numThreads; ++i)
        {
            threads.emplace_back([this]
                                 {
                while (true) {
                    std::function<void()> task;

                    {
                        std::unique_lock<std::mutex> lock(mutex);
                        condition.wait(lock, [this] { return stop || !tasks.empty(); });
                        if (stop ) // 线程池即将销毁
                            return;
                        task = std::move(tasks.front());
                        tasks.pop();
                    }

                    task();

                     {
                        std::unique_lock<std::mutex> lock(mutexCount);
                        --taskCount;
                        if (taskCount == 0) {
                            conditionCount.notify_all();
                        }
                    }
                } });
        }
    }

    void waitAllTasks()
    {
        std::unique_lock<std::mutex> lock(mutexCount);
        conditionCount.wait(lock, [this]
                            { return taskCount == 0; });
    }

    template <typename Func, typename... Args>
    void enqueue(Func &&func, Args &&...args)
    {
        {
            std::unique_lock<std::mutex> lock(mutexCount);
            ++taskCount;
        }

        {
            std::unique_lock<std::mutex> lock(mutex);
            tasks.emplace([func, args...]
                          { func(args...); });
        }
        condition.notify_one();
    }

    ~ThreadPool()
    {
        {
            std::unique_lock<std::mutex> lock(mutex);
            stop = true;
        }
        condition.notify_all();
        for (std::thread &thread : threads)
            thread.join();
    }

private:
    std::vector<std::thread> threads;
    std::queue<std::function<void()>> tasks;

    std::mutex mutex;
    std::condition_variable condition;

    size_t taskCount;
    std::condition_variable conditionCount;
    std::mutex mutexCount;

    bool stop;
};
```

主线程

```c++
int tasks_per_thread = (num_total_tasks + num_threads_ - 1) / num_threads_;
    for (int i = 0; i < num_threads_; i++)
    {
        int start = i * tasks_per_thread;
        int end = std::min((i + 1) * tasks_per_thread, num_total_tasks);
        pool_->enqueue([runnable, start, end, num_total_tasks]()
                       {
            for (int j = start; j < end; j++)
            {
                runnable->runTask(j, num_total_tasks);
            } });
    }
    pool_->waitAllTasks();
```



### Assignment 3：A Simple CUDA Renderer

这一个 assignment 学校本意是在云服务器上完成，所以给出的 Makefile 针对的平台都是 linux。我正好有台 win 本带 nvidia 1050 显卡，所以用了https://github.com/star-hengxing/cs149-xmake这个方案，用 xmake 构建项目

从[英伟达官网](https://www.nvidia.com/en-gb/geforce/10-series/)可以查到自己显卡的技术参数，但我的显卡应该是 4g 缓存的 1050，这里没有列出，内存带宽经我测试应该为112 GB/s, 了解内存带宽对后续实验分析性能很有帮助

<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240313145936583.png" alt="image-20240313145936583" style="zoom:50%;" />

#### Part 1：CUDA Warm-Up 1: SAXPY

part1 主要是分析使用 cuda 带来的性能提升，我们可以计算 cuda kernel 的耗时从而计算出吞吐量，与理论值 112 GB/s 进行比较从而发现性能瓶颈是什么造成的

```c
	double startTime = CycleTimer::currentSeconds();
    cudaMemcpy(device_x, xarray, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(device_y, yarray, N * sizeof(float), cudaMemcpyHostToDevice);
    // run CUDA kernel. (notice the <<< >>> brackets indicating a CUDA
    // kernel launch) Execution on the GPU occurs here.
    saxpy_kernel<<<blocks, threadsPerBlock>>>(N, alpha, device_x, device_y, device_result);
    // cudaDeviceSynchronize();
    cudaMemcpy(resultarray, device_result, N * sizeof(float), cudaMemcpyDeviceToHost);
	// end timing after result has been copied back into host memory
    double endTime = CycleTimer::currentSeconds();
```

首先，我们把`cudaMemcpy`函数包裹在记时函数内，从而包括了 kernel 运行以及数据在 CPU 和 GPU 之间传输的总时间。下图可以看出吞吐量只有5.7 GB/s 左右
<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240313150819421.png" alt="image-20240313150819421" style="zoom:50%;" />
而 Assignment 1 中串行执行的版本吞吐量都有 20 GB/s，显然不太对劲

<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240313150923887.png" alt="image-20240313150923887" style="zoom:50%;" />

接着，我们只计算 cuda kernel 运算的时间，不考虑 CPU 和 GPU 之间搬运数据的耗时

```c
    double startTime = CycleTimer::currentSeconds();
    // run CUDA kernel. (notice the <<< >>> brackets indicating a CUDA
    // kernel launch) Execution on the GPU occurs here.
    saxpy_kernel<<<blocks, threadsPerBlock>>>(N, alpha, device_x, device_y, device_result);
    cudaDeviceSynchronize();
    double endTime = CycleTimer::currentSeconds();
```

<img src="https://cdn.jsdelivr.net/gh/Real-Rio/pictures/img/image-20240313151233576.png" alt="image-20240313151233576" style="zoom:50%;" />

可以看出吞吐量大幅提升到了约 90 GB/s，超过了理论吞吐量的8成。通过以上两次测试可以发现运算的瓶颈来源于 CPU 和 GPU 之间的性能传输

#### Part 2: CUDA Warm-Up 2: Parallel Prefix-Sum

做这个 part 的时候遇到了一点小问题，windows 上无法使用 vscode 调试 cuda 程序，还好 WSL2 支持，关于 WSL2 安装 cuda 网上教程挺多，这里就不赘述了

为了调试要修改一下对应的 `xmake.lua` 文件，在对应 target 下增加以下判断，-g 表示将CPU代码(host)编译成可调式版本，-G表示将GPU代码(kernel)编译成可调式版本，然后 `xmake -m debug` 将 xmake 模式设为 debug 模式

```lua
 if is_mode("debug") then
        add_cuflags("-G -g")
    end
```

接着 vscode 拓展里安装 `Nsight Visual Studio Code Edition`，配置 `launch.json` 文件，根据自己实际情况修改

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "CUDA C++: Launch",
            "type": "cuda-gdb",
            "request": "launch",
            "program": "${workspaceFolder}/build/linux/x86_64/debug/cudaScan.exe"
        },
    ]
}
```



assignment 主页已经给出了并行执行 scan 的 c 代码，我们所需做的只是将其中的 `parallel_for` 部分改为 cuda 的 kernel 函数

```c
// c 代码
void exclusive_scan_iterative(int* start, int* end, int* output) {
    int N = end - start;
    memmove(output, start, N*sizeof(int));
    
    // upsweep phase
    for (int two_d = 1; two_d <= N/2; two_d*=2) {
        int two_dplus1 = 2*two_d;
        parallel_for (int i = 0; i < N; i += two_dplus1) {
            output[i+two_dplus1-1] += output[i+two_d-1];
        }
    }

    output[N-1] = 0;

    // downsweep phase
    for (int two_d = N/2; two_d >= 1; two_d /= 2) {
        int two_dplus1 = 2*two_d;
        parallel_for (int i = 0; i < N; i += two_dplus1) {
            int t = output[i+two_d-1];
            output[i+two_d-1] = output[i+two_dplus1-1];
            output[i+two_dplus1-1] += t;
        }
    }
}
```

```c
__global__ void upsweep(int two_dplus1, int two_d, int N, int *output)
{
    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;
    int i = thread_id * two_dplus1;
    if (i < N)
    {
        output[i + two_dplus1 - 1] += output[i + two_d - 1];
    }
}

__global__ void downsweep(int two_dplus1, int two_d, int N, int *output)
{
    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;
    int i = thread_id * two_dplus1;
    if (i < N)
    {
        int t = output[i + two_d - 1];
        output[i + two_d - 1] = output[i + two_dplus1 - 1];
        output[i + two_dplus1 - 1] += t;
    }
}

__global__ void setElementToZero(int *output, int N, int idx)
{
    output[idx] = 0;
}

void exclusive_scan(int *input, int N, int *result)
{
    int two_dplus1;
    int num;
    int blocks;

    N = nextPow2(N);

    for (int two_d = 1; two_d <= N / 2; two_d *= 2)
    {
        two_dplus1 = two_d * 2;
        num = (N + two_dplus1 - 1) / two_dplus1;
        blocks = (num + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

        if(num<THREADS_PER_BLOCK) {
            upsweep<<<1, num>>>(two_dplus1, two_d, N, result);
        }else {
            upsweep<<<blocks, THREADS_PER_BLOCK>>>(two_dplus1, two_d, N, result);
        }
        cudaDeviceSynchronize();
    }

    setElementToZero<<<1, 1>>>(result, N, N - 1);

    for (int two_d = N / 2; two_d >= 1; two_d /= 2)
    {
        two_dplus1 = two_d * 2;
        num = (N + two_dplus1 - 1) / two_dplus1;
        blocks = (num + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;

        if(num<THREADS_PER_BLOCK) {
                downsweep<<<1, num>>>(two_dplus1, two_d, N, result);

        }else {
            downsweep<<<blocks, THREADS_PER_BLOCK>>>(two_dplus1, two_d, N, result);

        }
        cudaDeviceSynchronize();
    }

    setElementToZero<<<1, 1>>>(result, N, 0);
}
```

要注意的是 `exclusive_scan` 并不是 kernel 函数，运行在 CPU 上，所以不能直接修改 result 数组的值，否则会报段错误。
